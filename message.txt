import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Load the breast cancer data
breast_ca = datasets.load_breast_cancer()

# Explore the data
print(breast_ca.keys())
print(breast_ca["DESCR"])
print(breast_ca["feature_names"])
print(breast_ca["target_names"])

# Assign input X and target y
X = breast_ca.data
y = breast_ca.target

# Feature scaling for SVC
X_scaler = StandardScaler().fit(X_train)
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Train and fit the model: SVC with Gaussian RBF Kernel
svc_clf = SVC(kernel="rbf", gamma="auto", C=1, max_iter=-1).fit(X_train_scaled, y_train)

# Evaluate the model's accuracy
print("For SVC with Gaussian RBF Kernel model:")
print("Train set accuracy = " + str(svc_clf.score(X_train_scaled, y_train)))
print("Test set accuracy = " + str(svc_clf.score(X_test_scaled, y_test)))

For SVC with Gaussian RBF Kernel model:
Train set accuracy = 0.9882629107981221
Test set accuracy = 0.972027972027972

# Train and fit the model: decision tree
tree_clf = DecisionTreeClassifier(max_depth=None).fit(X_train, y_train)

# Evaluate the model's accuracy
print("For Decision Tree model:")
print("Train set accuracy = " + str(tree_clf.score(X_train, y_train)))
print("Test set accuracy = " + str(tree_clf.score(X_test, y_test)))

# Determine features' importances
print("\nImportance of each feature:\n", tree_clf.feature_importances_)

For Decision Tree model:
Train set accuracy = 1.0
Test set accuracy = 0.951048951048951

Importance of each feature:
 [0.         0.         0.         0.         0.         0.
 0.         0.70264385 0.         0.         0.         0.
 0.         0.01277192 0.00155458 0.         0.         0.01702539
 0.         0.         0.0877369  0.12550655 0.         0.03452044
 0.00985664 0.         0.         0.00838371 0.         0.        ]

# Train and fit the model: random forest classifier
rforest_clf = RandomForestClassifier().fit(X_train, y_train)

# Evaluate the model's accuracy
print("For Random Forest model:")
print("Train set accuracy = " + str(rforest_clf.score(X_train, y_train)))
print("Test set accuracy = " + str(rforest_clf.score(X_test, y_test)))

# Determine features' importances
print("\nImportance of each feature:\n", rforest_clf.feature_importances_)

For Random Forest model:
Train set accuracy = 1.0
Test set accuracy = 0.972027972027972

Importance of each feature:
 [0.0203572  0.02322627 0.02598635 0.04133143 0.0066621  0.01541918
 0.07545279 0.16202326 0.00550662 0.00652847 0.00736119 0.00434392
 0.00986888 0.04103056 0.00350675 0.00480023 0.00873174 0.00562927
 0.00371145 0.00552231 0.10679671 0.02272615 0.11522445 0.09619906
 0.01558007 0.0118116  0.02579207 0.115044   0.00852107 0.00530486]

# Choose only X from column 7 and 20
X_selected = breast_ca.data[:, [7,20]]
print(X_selected[:3, :])

# Split the selected data into train and test set
X_train_select, X_test_select, y_train, y_test = train_test_split(X_selected, y, random_state=42)

# Train and fit the model: decision tree
rforest_clf = RandomForestClassifier(n_estimators=50).fit(X_train_select, y_train)

# Evaluate the model's accuracy
print("For Random Forest model (with selected X):")
print("Train set accuracy = " + str(rforest_clf.score(X_train_select, y_train)))
print("Test set accuracy = " + str(rforest_clf.score(X_test_select, y_test)))

For Random Forest model (with selected X):
Train set accuracy = 1.0
Test set accuracy = 0.916083916083916

# Plot the decision boundaries
def plot_decision_boundary(clf, X, y, cmap='Paired_r'):
    h = 0.003  # Boundary lines' resolution
    x_min, x_max = X[:,0].min() - 10*h, X[:,0].max() + 10*h
    y_min, y_max = X[:,1].min() - 10*h, X[:,1].max() + 10*h
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(12,9))
    plt.contourf(xx, yy, Z, cmap=cmap, alpha=0.25)  # Background
    plt.contour(xx, yy, Z, colors='k', linewidths=0.2)  # Boundary lines
    scatter = plt.scatter(X[:,0], X[:,1], c=y, cmap=cmap);  # Data points
    plt.xlabel(breast_ca.feature_names[7])
    plt.ylabel(breast_ca.feature_names[20])
    plt.legend(*scatter.legend_elements())

plot_decision_boundary(rforest_clf, X_selected, y)

# Help remind legend names
count = 0

for i in breast_ca["target_names"]:
    print(count, "means", i)
    count += 1
    
0 means malignant
1 means benign